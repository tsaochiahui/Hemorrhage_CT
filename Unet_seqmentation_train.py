import os
import cv2
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
import segmentation_models_pytorch as smp
import matplotlib.pyplot as plt
from PIL import Image
from tqdm import tqdm
import random

# === è‡ªè¨‚ Dataset é¡åˆ¥ ===
class SegDataset(Dataset):
    def __init__(self, image_dir, mask_dir, transform=None):
        self.image_dir = image_dir
        self.mask_dir = mask_dir
        self.image_ids = sorted(os.listdir(image_dir))
        self.transform = transform

    def __len__(self):
        return len(self.image_ids)

    def __getitem__(self, idx):
        img_path = os.path.join(self.image_dir, self.image_ids[idx])
        mask_path = os.path.join(self.mask_dir, self.image_ids[idx])

        image = cv2.imread(img_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)
        mask = (mask > 0).astype('float32')

        if self.transform:
            augmented = self.transform(image=image, mask=mask)
            image = augmented['image']
            mask = augmented['mask']

        image = np.transpose(image, (2, 0, 1)) / 255.0
        mask = np.expand_dims(mask, 0)
        return image.astype('float32'), mask.astype('float32')

# === å¯è¦–åŒ–å‡½æ•¸ ===
def visualize_prediction(model, dataset, index=0, device="cpu"):
    model.eval()
    image, mask = dataset[index]
    image_tensor = torch.tensor(image).unsqueeze(0).to(device)

    with torch.no_grad():
        pred = model(image_tensor)
        pred = torch.sigmoid(pred)
        pred = (pred > 0.5).float().squeeze().cpu().numpy()

    image_np = (image.transpose(1, 2, 0) * 255).astype(np.uint8)
    mask_np = mask.squeeze()

    plt.figure(figsize=(12, 4))
    plt.subplot(1, 3, 1); plt.imshow(image_np); plt.title("åŸå§‹å½±åƒ"); plt.axis("off")
    plt.subplot(1, 3, 2); plt.imshow(mask_np, cmap="gray"); plt.title("çœŸå¯¦å‡ºè¡€å€"); plt.axis("off")
    plt.subplot(1, 3, 3); plt.imshow(pred, cmap="gray"); plt.title("æ¨¡å‹é æ¸¬å€åŸŸ"); plt.axis("off")
    plt.show()

# === è©•ä¼°æŒ‡æ¨™å‡½æ•¸ ===
def calculate_metrics(model, dataloader, device="cpu"):
    model.eval()
    TP = TN = FP = FN = 0
    smooth = 1e-6

    for images, masks in dataloader:
        images = images.to(device)
        masks = masks.to(device)
        with torch.no_grad():
            outputs = model(images)
            preds = torch.sigmoid(outputs) > 0.5

        TP += ((preds == 1) & (masks == 1)).sum().item()
        TN += ((preds == 0) & (masks == 0)).sum().item()
        FP += ((preds == 1) & (masks == 0)).sum().item()
        FN += ((preds == 0) & (masks == 1)).sum().item()

    dice = (2 * TP) / (2 * TP + FP + FN + smooth)
    iou = TP / (TP + FP + FN + smooth)
    sensitivity = TP / (TP + FN + smooth)
    specificity = TN / (TN + FP + smooth)

    print("\nğŸ“Š Segmentation Metrics:")
    print(f"Dice Score:     {dice:.4f}")
    print(f"IoU:            {iou:.4f}")
    print(f"Sensitivity:    {sensitivity:.4f}")
    print(f"Specificity:    {specificity:.4f}")

# === ä¸»è¦ç¨‹å¼é–‹å§‹ ===
# è³‡æ–™è·¯å¾‘ï¼ˆè«‹æ”¹æˆä½ è‡ªå·±çš„ï¼‰
seg_path = "/Users/chia-huitsao/Documents/PHE-SICH-CT-IDS/Task3/seg_dataset"
train_dataset = SegDataset(f"{seg_path}/images/train", f"{seg_path}/masks/train")
val_dataset = SegDataset(f"{seg_path}/images/val", f"{seg_path}/masks/val")
train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False)

# æ¨¡å‹è¨­å®š
model = smp.Unet(encoder_name="resnet34", encoder_weights="imagenet", in_channels=3, classes=1)
device = torch.device("cpu")
model = model.to(device)

# Loss å‡½æ•¸è¨­å®š
bce_loss = nn.BCEWithLogitsLoss()
dice_loss = smp.losses.DiceLoss(mode='binary')

def combined_loss(pred, target):
    return bce_loss(pred, target) + dice_loss(pred, target)

optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)

# ç¢ºä¿å„²å­˜è³‡æ–™å¤¾å­˜åœ¨
os.makedirs("weights", exist_ok=True)

# è¨“ç·´è¿´åœˆ
num_epochs = 10
best_loss = float("inf")
loss_history = []

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0

    for images, masks in tqdm(train_loader, desc=f"Epoch {epoch+1}/{num_epochs}"):
        images = torch.tensor(images).to(device)
        masks = torch.tensor(masks).to(device)

        preds = model(images)
        loss = combined_loss(preds, masks)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        train_loss += loss.item()

    avg_loss = train_loss / len(train_loader)
    loss_history.append(avg_loss)
    print(f"âœ… Epoch {epoch+1}, Loss: {avg_loss:.4f}")

    # å„²å­˜æœ€ä½³æ¨¡å‹
    if avg_loss < best_loss:
        best_loss = avg_loss
        torch.save(model.state_dict(), "weights/unet_task3_best.pt")
        print(f"ğŸ’¾ å„²å­˜æœ€ä½³æ¨¡å‹ï¼ˆloss: {best_loss:.4f}ï¼‰")

    # æ¯å€‹ epoch éš¨æ©Ÿé¡¯ç¤ºä¸€å¼µé æ¸¬åœ–
    random_idx = random.randint(0, len(val_dataset) - 1)
    visualize_prediction(model, val_dataset, index=random_idx, device=device)

# æœ€çµ‚å„²å­˜æ¨¡å‹
torch.save(model.state_dict(), "weights/unet_task3_final.pt")
print("âœ… è¨“ç·´å®Œæˆï¼Œæ¨¡å‹å·²å„²å­˜ç‚º unet_task3_final.pt")

# ç•«å‡º Loss æ›²ç·š
plt.figure()
plt.plot(range(1, num_epochs + 1), loss_history, marker='o')
plt.title("Training Loss Curve")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.grid(True)
plt.show()

# è©•ä¼°æ¨¡å‹åœ¨é©—è­‰é›†çš„åˆ†å‰²æŒ‡æ¨™
calculate_metrics(model, val_loader, device=device)
